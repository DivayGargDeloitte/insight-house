{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43610957-4d9b-44bf-baad-26010999ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#import os\n",
    "import pickle\n",
    "#import joblib\n",
    "from google.cloud import storage\n",
    "#import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8477c6b-8f2a-471f-aa32-623664c70a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"gs://lifesight-data-table/MLops/train_data.csv\")\n",
    "y_train = train_data['y_train']\n",
    "X_train = train_data.drop(columns=['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2da110f4-7578-4ef2-832e-260934174f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=1, normalize=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(normalize=True,fit_intercept=True,n_jobs=1)\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb74b094-5433-47fd-a803-60697bae1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.sav'\n",
    "joblib.dump(lr,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b137c741-d97b-4281-aaaa-a2d62cdfd52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'lifesight-data-table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73916597-ae8c-4670-b4ce-400ee0fd8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage.Client().bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(str('MLops/')+filename)\n",
    "blob.upload_from_filename(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a65c06-676a-444c-bcb0-7a7044766842",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gs://lifesight-data-table/MLops/model.sav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17056/2573702943.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gs://lifesight-data-table/MLops/model.sav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gs://lifesight-data-table/MLops/model.sav'"
     ]
    }
   ],
   "source": [
    "filename = 'gs://lifesight-data-table/MLops/model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e00c10-8c62-49ba-b18c-889692baed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc827727-b539-4e6e-819b-e6a59fea9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.cloud import storage\n",
    "#from sklearn.externals import joblib\n",
    "#from tempfile import TemporaryFile\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket_name=<bucket name>\n",
    "model_bucket='model.joblib'\n",
    "\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "#select bucket file\n",
    "blob = bucket.blob(model_bucket)\n",
    "with TemporaryFile() as temp_file:\n",
    "    #download blob into temp file\n",
    "    blob.download_to_file(temp_file)\n",
    "    temp_file.seek(0)\n",
    "    #load into joblib\n",
    "    model=joblib.load(temp_file)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
